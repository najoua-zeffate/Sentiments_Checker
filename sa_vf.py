# -*- coding: utf-8 -*-
"""SA_VF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19gl8lapSHcm0Okub9V4Avl2BDe7rNVli

# **Sentiment Analysis**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/Booking_data_eng.csv')

!pip install textblob

from textblob import TextBlob

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

SA=[]
sentiment = SentimentIntensityAnalyzer()
for i in range(0,len(df)):
  text_1=df.loc[df.index[i], 'Commentaire']
  sent_1 = sentiment.polarity_scores(text_1)
  SA.append(sent_1)

"""### **Adding Some constraints to give rate**"""

SA1=[]
sentiment = SentimentIntensityAnalyzer()
for i in range(0,len(df)):
  text_1=df.loc[df.index[i], 'Commentaire']
  sent_1 = sentiment.polarity_scores(text_1)
  if sent_1['compound']>=0:
    SA1.append(sent_1['compound']*10)
  elif sent_1['compound']>=-0.5 and sent_1['compound']<0 :
     SA1.append((sent_1['compound']*10)+5)
  else:
     SA1.append((sent_1['compound']*10)+10)

df3 = df2.assign(SentAC_Rat=SA1)

df3.head(3)

df3

SA

len(df[df['Rating'] < 5])

df[df['Rating'] < 5]

df.loc[df.index[175], 'Commentaire']

df3.loc[df3.index[175], 'SentAC']

df3.loc[df3.index[175], 'SentAC_Rat']

df.loc[df.index[175], 'Rating']

"""# **Trained model language**

**Importation des données**
"""

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

! mkdir ~/.kaggle

! cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/kaggle.json

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download basilb2s/language-detection

! unzip language-detection

import pandas as pd
import numpy as np
import re
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter("ignore")

data=pd.read_csv('/content/Language Detection.csv')

data["Language"].value_counts()

X = data["Text"]
y = data["Language"]

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

# creating a list for appending the preprocessed text
data_list = []
# iterating through all the text
for text in X:
       # removing the symbols and numbers
        text = re.sub(r'[!@#$(),n"%^*?:;~`0-9]', ' ', text)
        text = re.sub(r'[[]]', ' ', text)
        # converting the text to lower case
        text = text.lower()
        # appending to data_list
        data_list.append(text)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X = cv.fit_transform(data_list).toarray()
X.shape # (10337, 39419)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
ac = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Accuracy is :",ac)
# Accuracy is : 0.9772727272727273

plt.figure(figsize=(15,10))
sns.heatmap(cm, annot = True)
plt.show()

def predict(text):
     x = cv.transform([text]).toarray() # converting text to bag of words model (Vector)
     lang = model.predict(x) # predicting the language
     lang = le.inverse_transform(lang) # finding the language corresponding the the predicted value
     return(lang[0])
     #print("The langauge is in",lang[0]) # printing the language

predict(df.loc[df.index[0], 'Commentaire'])

lnt=[]
for i in range(0,len(df)):
  lnt.append(predict(df.loc[df.index[i], 'Commentaire']))

df4 = df3.assign(langtr=lnt)

df4.to_csv('/content/drive/MyDrive/Language detection/OurData1.csv')

"""# **Topic Model /BERTopic**"""

!pip install bertopic[visualization] --quiet

"""**Imports**"""

import numpy as np
import pandas as pd
from copy import deepcopy
from bertopic import BERTopic

"""**Load data**"""

topic_data=df4[df4['langtr'] == "English"]

"""# **Data Preprocessing**"""

print(topic_data['Commentaire'])

import re

data=topic_data

data['Commentaire']=data['Commentaire'].replace(to_replace ='@(\w+)', value = '', regex = True)

data['Commentaire']=data['Commentaire'].apply(lambda x: re.split('http.*', str(x))[0])

#Elemine de caractère speciaux
data['Commentaire']=data['Commentaire'].replace(to_replace ='[^a-zA-Z0-9\'\s]', value = '', regex = True)

"""**Transformation to lower case**"""

data['Commentaire']=data['Commentaire'].str.lower()

"""### **Tokenisation**"""

import nltk

nltk.download('punkt')

from nltk import word_tokenize

L= len(data)

"""**Creation d'une liste dont on va stoquer nos token**"""

test=[]
for i in range(0,L):
  test.append((word_tokenize(data.loc[data.index[i], 'Commentaire'])))

data['Commentaire']=test

data1=data.assign(ComTok=test)

data1['Commentaire']

print((data1.loc[data.index[0], 'ComTok']))

"""# **Lemmatisation**"""

!pip install spacy

import spacy
nlp = spacy.load("en_core_web_sm")

LL=[]
for j in range(0,len(data1)):
  k=0
  #print(j)
  L=data1.loc[data.index[j], 'ComTok']
  #print(L)
  for mot in L:
    m=nlp(mot)
    for i in m:
      L[k]=i.lemma_
      #L[k].replace(mot,i.lemma_)
    k+=1
  LL.append(L)

data2 = data1.assign(ComLem=LL)

"""**Drop Stop Words**"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')
data2['ComLem'] = data2['ComLem'].apply(lambda words: [word for word in words if word not in stop])
#df[7]= df['7'].apply(lambda words: [word for word in words if word not in stop])

print((data2.loc[data.index[0], 'ComLem']))

print(data2['ComLem'])

for i in range(0,len(data1)):
   data2.loc[data2.index[i], 'ComLem']=' '.join((data2.loc[data2.index[i], 'ComLem']))

print(data2['ComLem'])

data2.to_csv('/content/drive/MyDrive/Language detection/lemEn2.csv')

"""# **Starting BerTopic processing**"""

docs = list(data2.loc[:, "ComLem"].values)

print(docs[:5])

docs[:5]

"""**Creating Topics**"""

model = BERTopic(language="english")

topics, probs = model.fit_transform(docs)

"""**We can then extract most frequent topics:**"""

model.get_topic_freq()

"""**Get Individual Topics**"""

model.get_topic(8)

"""**Visualize Topics**"""

model.visualize_topics()

model.visualize_topics()

model.visualize_barchart()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from umap import UMAP
 
import matplotlib
import matplotlib.pyplot as plt
 
# %matplotlib inline
 
# Prepare data for plotting
embeddings = model._extract_embeddings(docs, method="document")
umap_model = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit(embeddings)
dff = pd.DataFrame(umap_model.embedding_, columns=["x", "y"])
dff["topic"] = topics

top_n = 10
fontsize = 12
 
# Slice data
to_plot = dff.copy()
to_plot[dff.topic >= top_n] = -1
outliers = to_plot.loc[to_plot.topic == -1]
non_outliers = to_plot.loc[to_plot.topic != -1]
 
# Visualize topics
cmap = matplotlib.colors.ListedColormap(['#FF5722', # Red
                                        '#03A9F4', # Blue
                                        '#4CAF50', # Green
                                        '#80CBC4', # FFEB3B
                                        '#673AB7', # Purple
                                        '#795548', # Brown
                                        '#E91E63', # Pink
                                        '#212121', # Black
                                        '#00BCD4', # Light Blue
                                        '#CDDC39', # Yellow/Red
                                        '#AED581', # Light Green
                                        '#FFE082', # Light Orange
                                        '#BCAAA4', # Light Brown
                                        '#B39DDB', # Light Purple
                                        '#F48FB1', # Light Pink
                                        ])
 
# Visualize outliers + inliers
fig, ax = plt.subplots(figsize=(15, 15))
scatter_outliers = ax.scatter(outliers['x'], outliers['y'], c="#E0E0E0", s=1, alpha=.3)
scatter = ax.scatter(non_outliers['x'], non_outliers['y'], c=non_outliers['topic'], s=1, alpha=.3, cmap=cmap)
 
# Add topic names to clusters
centroids = to_plot.groupby("topic").mean().reset_index().iloc[1:]
for row in centroids.iterrows():
   topic = int(row[1].topic)
   text = f"{topic}: " + "_".join([x[0] for x in model.get_topic(topic)[:3]])
   ax.text(row[1].x, row[1].y*1.01, text, fontsize=fontsize, horizontalalignment='center')
 
ax.text(0.99, 0.01, f"BERTopic - Top {top_n} topics", transform=ax.transAxes, horizontalalignment="right", color="black")
plt.xticks([], [])
plt.yticks([], [])
plt.savefig("BERTopic_Example_Cluster_Plot.png")
plt.show()

"""# **Language Detection model**

**Model prédefine**
"""

!pip install pycld2

lg=[]
import pycld2 as cld2
for i in range(0,len(df)):
    text_content = df.loc[df.index[i], 'Commentaire']
    _, _, _, detected_language = cld2.detect(text_content,  returnVectors=True)
    lg.append(detected_language)
#print(detected_language)

df4 = df.assign(lang=lg)

df4

"""# **Test language**"""

df5[df5['langtr'] == "English"]

len(df5[df5['langtr'] == "English"])

len(df5[df5['langtr'] == "French"])

len(df5[df5['langtr'] == "Spanish"])

7701+3732+2055

len(df5[df5['langtr'] == "Arabic"])

df5[df5['langtr'] == "Arabic"]

len(df5[df5['langtr'] == "Hindi"])

len(df5[df5['langtr'] == "Italian"])

len(df5[df5['langtr'] == "Turkish"])

df5[df5['langtr'] == "Turkish"]